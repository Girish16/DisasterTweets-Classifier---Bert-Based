{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.version.VERSION)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnwK3P7IcS8L","outputId":"6faee0f6-ebe7-4aaf-a391-40de4222f099","execution":{"iopub.status.busy":"2023-07-19T15:20:37.428683Z","iopub.execute_input":"2023-07-19T15:20:37.429218Z","iopub.status.idle":"2023-07-19T15:20:52.218207Z","shell.execute_reply.started":"2023-07-19T15:20:37.429180Z","shell.execute_reply":"2023-07-19T15:20:52.217163Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"2.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3ywmwaacc9l","outputId":"b97543d4-c248-4071-9aa4-e74381f47b6a","execution":{"iopub.status.busy":"2023-07-19T15:22:20.173610Z","iopub.execute_input":"2023-07-19T15:22:20.173977Z","iopub.status.idle":"2023-07-19T15:22:24.455115Z","shell.execute_reply.started":"2023-07-19T15:22:20.173944Z","shell.execute_reply":"2023-07-19T15:22:24.453862Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'models'...\nremote: Enumerating objects: 2650, done.\u001b[K\nremote: Counting objects: 100% (2650/2650), done.\u001b[K43% (1140/2650)\u001b[K\nremote: Compressing objects: 100% (2311/2311), done.\u001b[K\nremote: Total 2650 (delta 505), reused 1389 (delta 306), pack-reused 0\u001b[K\nReceiving objects: 100% (2650/2650), 34.02 MiB | 28.93 MiB/s, done.\nResolving deltas: 100% (505/505), done.\nNote: switching to '400d68abbccda2f0f6609e3a924467718b144233'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# install requirements to use tensorflow/models repository\n!pip install -Uqr models/official/requirements.txt\n# you may have to restart the runtime afterwards","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bqnr2KTmcrhZ","outputId":"1379aa6a-e3fe-4e3f-dcfa-02fb05f4eb2d","execution":{"iopub.status.busy":"2023-07-19T15:22:24.457637Z","iopub.execute_input":"2023-07-19T15:22:24.458007Z","iopub.status.idle":"2023-07-19T15:24:02.227137Z","shell.execute_reply.started":"2023-07-19T15:22:24.457968Z","shell.execute_reply":"2023-07-19T15:24:02.225846Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.25.1 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.621.222118 requires jupyter-server~=1.16, but you have jupyter-server 2.6.0 which is incompatible.\ncloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.94.0 which is incompatible.\ncudf 23.6.1 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cuda 23.6.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndistributed 2023.3.2.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nfeaturetools 1.26.0 requires pandas<2.0.0,>=1.5.0, but you have pandas 2.0.3 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-cloud-bigquery<3.0.0dev,>=1.15.0, but you have google-cloud-bigquery 3.11.3 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.25.1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.25.1 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.1 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ntensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.1 which is incompatible.\ntensorflowjs 3.15.0 requires tensorflow-hub<0.13,>=0.7.0, but you have tensorflow-hub 0.14.0 which is incompatible.\nwoodwork 0.24.0 requires pandas<2.0.0,>=1.4.3, but you have pandas 2.0.3 which is incompatible.\nydata-profiling 4.3.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.25.1 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport sys\nsys.path.append('models')\nfrom official.nlp.data import classifier_data_lib\nfrom official.nlp.bert import tokenization\nfrom official.nlp import optimization","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTVLUe-VcuOJ","outputId":"a030ead7-f69a-46a4-cbaf-dd22f5192c2e","execution":{"iopub.status.busy":"2023-07-19T15:24:02.229976Z","iopub.execute_input":"2023-07-19T15:24:02.230644Z","iopub.status.idle":"2023-07-19T15:24:02.296634Z","shell.execute_reply.started":"2023-07-19T15:24:02.230601Z","shell.execute_reply":"2023-07-19T15:24:02.294793Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mofficial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_data_lib\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mofficial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenization\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mofficial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimization\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'official'"],"ename":"ModuleNotFoundError","evalue":"No module named 'official'","output_type":"error"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"ZOwVQ7-Oc6UE","execution":{"iopub.status.busy":"2023-07-19T15:24:02.297284Z","iopub.status.idle":"2023-07-19T15:24:02.297631Z","shell.execute_reply.started":"2023-07-19T15:24:02.297462Z","shell.execute_reply":"2023-07-19T15:24:02.297478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')","metadata":{"id":"rkV36gPReIXU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.target.plot(kind='hist', title='Target Distribution')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"g5wRBLNCeEjE","outputId":"7efdb0d3-214f-49f9-ecad-e781d7d4e2ba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, valid_df = train_test_split(df, random_state = 42, train_size = 0.7, stratify = df.target.values)\n","metadata":{"id":"PeExUB4if9Fm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device('/cpu:0'):\n  train_data = tf.data.Dataset.from_tensor_slices((train_df['text'].values, train_df['target'].values))\n  valid_data = tf.data.Dataset.from_tensor_slices((valid_df['text'].values, valid_df.target.values))","metadata":{"id":"CO-frGvogGqL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nEach line of the dataset is composed of the review text and its label\n- Data preprocessing consists of transforming text to BERT input features:\ninput_word_ids, input_mask, segment_ids\n- In the process, tokenizing the text is done with the provided BERT model tokenizer\n\"\"\"\n\nlabel_list = [0,1]# Label categories\nmax_seq_length = 128 # maximum length of (token) input sequences\ntrain_batch_size = 32\n\n# Get BERT layer and tokenizer:\n# More details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable = True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n","metadata":{"id":"esS049ibgAdh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This provides a function to convert row to input features and label\n\ndef to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):\n  example= classifier_data_lib.InputExample(guid = None, text_a = text.numpy(), text_b = None, label = label.numpy())\n  feature = classifier_data_lib.convert_single_example(0, example, label_list, max_seq_length, tokenizer)\n\n  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)\n","metadata":{"id":"b_YDj3Ifh7lK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_feature_map(text, label):\n  input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature, inp=(text, label),\n                                                              Tout= [tf.int32, tf.int32, tf.int32, tf.int32])\n  input_ids.set_shape([max_seq_length])\n  input_mask.set_shape([max_seq_length])\n  segment_ids.set_shape([max_seq_length])\n  label_id.set_shape([])\n\n  x = {\n      'input_word_ids' : input_ids,\n      'input_mask' : input_mask,\n      'input_type_ids' : segment_ids\n  }\n\n  return (x, label_id)\n","metadata":{"id":"ErubpIU3h7dO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device('/cpu:0'):\n  # train\n  train_data = (train_data.map(to_feature_map,\n                               num_parallel_calls = tf.data.experimental.AUTOTUNE)\n  .shuffle(1000)\n  .batch(32, drop_remainder = True)\n  .prefetch(tf.data.experimental.AUTOTUNE))\n\n  # valid\n\n  valid_data = (valid_data.map(to_feature_map, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n  .batch(32, drop_remainder = True)\n  .prefetch(tf.data.experimental.AUTOTUNE))","metadata":{"id":"cy8PVC8nkqWJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train data spec\ntrain_data.element_spec","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NPJc2v64lGs_","outputId":"1720536f-b97a-4db2-fb83-79ca6f8fcd22","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid data spec\nvalid_data.element_spec","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oXSDMaDylJ7f","outputId":"010aa49c-5cfa-4c0c-fdf8-b2f1a4354fb8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\ndef create_model():\n  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length, ), dtype=tf.int32,\n                                         name = \"input_word_ids\")\n  input_mask = tf.keras.layers.Input(shape=(max_seq_length, ), dtype=tf.int32,\n                                     name= \"input_mask\")\n  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length, ), dtype=tf.int32,\n                                     name= \"input_type_ids\")\n  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n\n  drop = tf.keras.layers.Dropout(0.4)(pooled_output)\n  output = tf.keras.layers.Dense(1, activation = 'sigmoid', name= 'output')(drop)\n\n  model = tf.keras.Model(\n      inputs={\n          'input_word_ids' : input_word_ids,\n          'input_mask' : input_mask,\n          'input_type_ids': input_type_ids\n      },\n      outputs=output\n  )\n  return model\n","metadata":{"id":"tfwyIF38lKhy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()\nmodel.compile(optimizer= tf.keras.optimizers.Adam(learning_rate = 2e-5),\n              loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics= [tf.keras.metrics.BinaryAccuracy()])\nmodel.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6Np9oKslMz8","outputId":"750ab5ca-cc6c-4f0c-aacf-8627b9169f10","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model=model, show_shapes=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422},"id":"HL5_1jMjlavs","outputId":"77cd2edf-1b99-4b10-fb54-3405b036df66","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\nepochs = 8\nhistory = model.fit(train_data, validation_data= valid_data\n                    , epochs = epochs, verbose=1)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zlmbwp6slfZE","outputId":"d9fdb243-e850-4ab4-8d9a-376c0564dbef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save the weights\nmodel.save('./saved_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testData= pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsample_example = testData['text']\ntest_data = tf.data.Dataset.from_tensor_slices((sample_example, [0]*len(sample_example)))\ntest_data = (test_data.map(to_feature_map).batch(1))\npreds = model.predict(test_data)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiWzyUVDBNZ_","outputId":"de22435b-1aed-47f8-e520-1d86f347fdda","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testData","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = [1 if pred >= 0.5 else 0 for pred in preds]\nfinal_data = pd.DataFrame({'id': testData['id'], 'pred': preds})","metadata":{"id":"2K224-6QCqAt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data.to_csv('test_data.csv', index = False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}